{"id":"speak-0vs","title":"Add per-event-type sound effects","description":"From cc-hooks by husniadil. Different Claude Code hook events get distinct audio cues: a chime for task completion, a click for tool use, a warning tone for errors, etc. cc-hooks maps specific hook events (PreToolUse, PostToolUse, UserPromptSubmit, Stop, Notification) to bundled sound effect files. This helps the user understand what's happening without needing to listen to full text. Could be implemented as pre-recorded PCM samples injected into the audio stream before spoken text.","status":"open","priority":3,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-21T00:27:10Z","created_by":"Trillium Smith","updated_at":"2026-02-21T00:27:10Z","labels":["enhancement","research-finding"]}
{"id":"speak-1xv","title":"Use NLTK/Stanza sentence tokenization instead of regex","description":"From RealtimeTTS by KoljaB. Our current clause splitting uses regex on punctuation marks (.\\!?,;:--). RealtimeTTS uses NLTK or Stanza sentence tokenizers which handle edge cases much better: abbreviations (Dr., U.S.A.), decimal numbers (3.14), URLs, quoted speech, and other tricky patterns. NLTK's punkt tokenizer is lightweight and well-tested. This would improve clause boundary detection without adding heavy dependencies.","status":"open","priority":3,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-21T00:26:58Z","created_by":"Trillium Smith","updated_at":"2026-02-21T00:26:58Z","labels":["enhancement","research-finding"]}
{"id":"speak-2w6","title":"Add voice formula blending support","description":"From RealtimeTTS's KokoroEngine. Kokoro supports blending multiple voice profiles using weighted formulas like '0.3*af_sarah + 0.7*am_adam'. This creates unique voice textures without needing additional voice models. RealtimeTTS implements this by loading multiple voice tensors and combining them with the specified weights. Could expose this via --voice '0.5*af_heart+0.5*am_adam' syntax.","status":"open","priority":4,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-21T00:27:04Z","created_by":"Trillium Smith","updated_at":"2026-02-21T00:27:04Z","labels":["enhancement","research-finding"]}
{"id":"speak-36y","title":"Implement CoreAudio device change listener via ctypes","description":"Add background thread with CFRunLoop listening for kAudioHardwarePropertyDefaultOutputDevice and kAudioHardwarePropertyDevices changes. On change, kill ffplay so next write restarts on new device. Pure Python, no deps. Working prototype at /tmp/test_coreaudio_listener.py. Integration point: FFPlayStream or a new lib/speakd/audio_monitor.py module.","status":"open","priority":2,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-22T07:13:03Z","created_by":"Trillium Smith","updated_at":"2026-02-22T07:13:03Z","labels":["audio","enhancement"]}
{"id":"speak-3pr","title":"Diagnose tone-to-speech gap: run benchmark and implement fix","description":"CONTEXT: Bead speak-788. Users hear a 1-5 second gap between the caller tone and speech starting. A prefetch optimization exists but doesn't eliminate the gap. Timing instrumentation is in place (grep TIMING and prefetch_first_chunk in /tmp/speak-$USER.sock.log).\n\nSTEP 1 — RUN THE BENCHMARK:\n  uv run --python 3.14 --with kokoro-onnx -- python3 bin/bench-first-chunk\nThis tests Kokoro create_stream directly (no daemon) for tiny/short/medium/long text. It separately times phonemization (espeak) and first-chunk ONNX inference. Compare results to daemon prefetch times in the logs.\n\nSTEP 2 — INTERPRET RESULTS:\n- If tiny (1 word) first_chunk is fast (\u003c200ms) but long is slow → chunk size IS the issue, fix by splitting text into clauses before synthesis\n- If tiny first_chunk is also slow (\u003e500ms) → ONNX inference has fixed startup cost, chunk size is NOT the issue\n- If phonemize dominates the time → espeak is the bottleneck, consider caching phonemes or pre-phonemizing at enqueue time\n- If benchmark is fast but daemon is slow → daemon adds overhead, look at cache lookups, voice pool, or asyncio scheduling\n\nSTEP 3 — IMPLEMENT THE FIX based on findings:\n- If chunk size: split text via existing split_clauses() (lib/speakd/text.py) before passing to create_stream, synthesize first clause only for prefetch\n- If espeak: pre-phonemize at enqueue time or cache phonemization results\n- If ONNX fixed cost: extend the caller tone dynamically to cover synthesis time (loop tone until prefetch resolves)\n- If daemon overhead: identify and eliminate the overhead\n\nSTEP 4 — VERIFY:\n- Restart daemon (rm -rf lib/speakd/__pycache__ first!)\n- Enqueue a few messages: speak --enqueue 'test message here'\n- Check timing logs: grep TIMING /tmp/speak-$USER.sock.log\n- await_prefetch should be near 0 (prefetch finishes during tone)\n- The gap(tone-\u003espeech) should be under 500ms\n\nSTEP 5 — Fix the broken first_speech metric in timing logs. Currently on_first_write callback fires AFTER write_pcm returns (which blocks for full chunk duration). Should fire after the first 0.25s sub-chunk write. See renderer.py line 119.\n\nSTEP 6 — Update bead speak-788 with findings and close if fixed.\n\nKEY FILES:\n- bin/bench-first-chunk — standalone benchmark (run outside daemon)\n- lib/speakd/playback.py — _worker method with timing instrumentation\n- lib/speakd/renderer.py — prefetch_first_chunk, render_speech, on_first_write callback\n- lib/speakd/text.py — split_clauses (existing clause splitter)\n- lib/speakd/synthesis.py — SynthesisEngine with cache","status":"open","priority":1,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-23T23:18:19Z","created_by":"Trillium Smith","updated_at":"2026-02-23T23:18:19Z","labels":["investigation","latency","playback"]}
{"id":"speak-3r3","title":"Fix dual ffplay: bg_upgrade create_task gets Future not coroutine","description":"Two ffplay processes run simultaneously causing overlapping audio. Root cause: renderer.py line 94 does asyncio.create_task(loop.run_in_executor(...)) but run_in_executor returns a Future, not a coroutine. create_task requires a coroutine. The error 'a coroutine was expected, got Future' propagates to the playback worker's except block, which calls ffplay.kill() in the finally. But ffplay has buffered audio still playing. Next write spawns a new ffplay while old one plays out its buffer. Fix: either wrap in an async def or use asyncio.ensure_future() instead of create_task(). Affects bg_task_tracker callback.","status":"closed","priority":1,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-22T06:52:57Z","created_by":"Trillium Smith","updated_at":"2026-02-22T06:56:05Z","closed_at":"2026-02-22T06:56:05Z","close_reason":"Closed","labels":["audio","bug"]}
{"id":"speak-5cb","title":"Add SQLite event persistence with retry logic","description":"From cc-hooks by husniadil. Currently our playback queue is in-memory only - if the daemon crashes, pending items are lost. cc-hooks uses SQLite with a proper state machine (PENDING -\u003e PROCESSING -\u003e COMPLETED/FAILED), atomic claim-based processing, configurable max retries, and delay between attempts. Also enables event history for debugging and replay. Consider storing queue items in SQLite for durability across daemon restarts.","status":"open","priority":2,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-21T00:26:51Z","created_by":"Trillium Smith","updated_at":"2026-02-21T00:26:51Z","labels":["enhancement","reliability","research-finding"]}
{"id":"speak-69m","title":"Add COMPLETED convention for brief agent summaries","description":"From Benny Cheung's AgentVoices project. Agents append standardized ~12-word past-tense summaries (e.g. 'COMPLETED: Refactored authentication module and added unit tests'). Claude Code hooks parse these lines via regex and speak only the brief summary, not the full output. This constrains what gets spoken to naturally-phrased outcomes. Could be implemented as a hook that detects COMPLETED lines and pipes them to speak --enqueue.","notes":"Partially addressed: the hook pipeline now summarizes Claude output via Haiku before speaking, which achieves a similar result. The formal COMPLETED convention (agents explicitly outputting structured summaries) remains a future enhancement.","status":"open","priority":3,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-21T00:26:45Z","created_by":"Trillium Smith","updated_at":"2026-02-21T01:20:14Z","labels":["enhancement","research-finding"]}
{"id":"speak-6ya","title":"Add audio device selection and disconnection detection","description":"When Bluetooth headphones disconnect, macOS keeps them as default output and audio goes nowhere. Need: 1) --audio-device flag to target specific output, 2) detect when current device disconnects and fall back to available device, 3) optionally auto-restart ffplay on device change. SwitchAudioSource (now installed) can list/switch devices. Related to speak-auf (stale audio bug).","notes":"Research complete: Best approach is CoreAudio AudioObjectAddPropertyListener via ctypes. No external deps. Listen on kAudioHardwarePropertyDefaultOutputDevice (0x644f7574) and kAudioHardwarePropertyDevices (0x64657623). Callback fires on device change. Run CFRunLoop in background thread. 70-line working prototype at /tmp/test_coreaudio_listener.py. On device change: kill ffplay, next write restarts with new device.","status":"open","priority":2,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-22T07:03:56Z","created_by":"Trillium Smith","updated_at":"2026-02-22T07:08:46Z","labels":["audio","bug","enhancement"]}
{"id":"speak-788","title":"Audible gap persists: TTS first-chunk synthesis slower than caller tone","description":"PROBLEM: Prefetch optimization (speak-sm9) helps but doesn't eliminate the gap. TTS first-chunk synthesis takes 800-5300ms while caller tones are only 400-960ms. The difference (await_prefetch in logs) is audible dead air after the tone ends.\n\nTIMING DATA (from instrumented logs):\n  q#1:  tone=960ms  prefetch=1969ms  await_after_tone=1009ms\n  q#3:  tone=384ms  prefetch=836ms   await_after_tone=451ms\n  q#17: tone=710ms  prefetch=3377ms  await_after_tone=2666ms\n  q#18: tone=382ms  prefetch=4582ms  await_after_tone=4200ms\n  q#19: tone=778ms  prefetch=2992ms  await_after_tone=2213ms\n  q#20: tone=563ms  prefetch=5295ms  await_after_tone=4563ms\n\nPOSSIBLE APPROACHES:\n1. Extend tone/filler: loop or pad the caller tone until prefetch completes. Play a sustained ambient tone or gentle pulse that fills the gap dynamically.\n2. Pre-phonemize: separate phonemization from ONNX inference. Phonemize eagerly when enqueued (before playback worker picks it up) so prefetch only does inference.\n3. Reduce first-chunk size: if Kokoro generates one giant chunk for short text, split the input so the first chunk is tiny and fast.\n4. Warm cache: on enqueue, immediately start caching the synthesis result in background so by the time the worker plays the tone, audio is already cached.\n5. Accept the gap but fill it: play a brief 'thinking' sound or white noise burst that feels intentional rather than broken.","notes":"ROOT CAUSE INVESTIGATION IN PROGRESS. Current hypothesis (unverified): Kokoro create_stream yields entire utterance as single chunk when phonemes \u003c 510. But user correctly points out this is unverified. Benchmark script at bin/bench-first-chunk ready to run. Need to determine whether slowness is: (A) Kokoro generating one giant chunk, (B) slow espeak phonemization, (C) slow ONNX inference regardless of size, or (D) daemon overhead (cache lookup, voice pool, etc). TIMING DATA from daemon logs shows prefetch takes 800-5300ms. The first_speech metric in logs is broken (fires after full write_pcm, not first sub-chunk).","status":"open","priority":2,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-23T23:05:15Z","created_by":"Trillium Smith","updated_at":"2026-02-23T23:17:55Z","labels":["latency","playback","ux"],"dependencies":[{"issue_id":"speak-788","depends_on_id":"speak-3pr","type":"blocks","created_at":"2026-02-23T15:18:23Z","created_by":"Trillium Smith","metadata":"{}"}]}
{"id":"speak-9ln","title":"Replace ffplay with direct audio playback library","description":"Replace the ffplay subprocess-based audio playback in speak-daemon with a direct Python audio library. The core problem: ffplay has no readiness signal — we cannot know when its SDL audio device is ready to accept samples, causing truncation at the start of playback (see speak-auf). A direct audio library gives us synchronous device-open, so we know the device is ready before writing the first sample.\n\nRequirements (priority order):\n1. FAST START — minimal latency from device-open to first-sample-plays. Must provide synchronous open or readiness callback so we know when the device is ready.\n2. LOW MEMORY — lightweight, minimal dependencies. The daemon already loads Kokoro ONNX + numpy; we want to keep the audio layer slim.\n3. macOS Apple Silicon ARM64 support (primary platform).\n4. Streaming — must support writing PCM chunks incrementally as they arrive from synthesis. We do NOT have a complete buffer before playback starts.\n5. Must support mono 24kHz int16 PCM (our Kokoro output format).\n6. Audio device change detection (nice to have — currently handled by recycling ffplay).\n\nCurrent architecture (lib/speakd/ffplay.py):\n- FFPlayStream class wraps asyncio subprocess\n- Pipes raw PCM (s16le, 24kHz, mono) to ffplay stdin\n- 100ms silence prime buffer to work around startup truncation (insufficient)\n- Writes in 0.25s chunks for backpressure pacing\n- Kills/respawns ffplay between queue batches to handle device changes\n\nThe replacement needs to be a drop-in for FFPlayStream — same interface: ensure_running(), write_pcm(bytes), kill(). The PlaybackQueue in lib/speakd/playback.py is the consumer.\n\nResearch completed 2026-02-22. Top candidates: sounddevice (recommended), miniaudio, SoundCard, rtmixer, PvSpeaker, PyAudio.","notes":"RECOMMENDATION (2026-02-22 research complete):\n\nRANKING BY VIABILITY:\n\n1. **sounddevice** (RECOMMENDED FIRST) — P1\n   - Synchronous device open SOLVES the readiness problem\n   - Blocking write() provides natural backpressure (like ffplay pipe)\n   - RawOutputStream works without numpy for raw PCM\n   - Tiny package (109 KB), bundled PortAudio, macOS ARM64 wheel\n   - Battle-tested PortAudio backend\n   - Simple migration path: FFPlayStream.write_pcm() → stream.write()\n   - Can integrate with asyncio via run_in_executor()\n\n2. **miniaudio** (STRONG SECOND) — P2\n   - Pull-based generator model is elegant, inherently ready-aware\n   - Direct CoreAudio backend (no PortAudio middleman)\n   - Small package (330 KB), macOS ARM64 wheels\n   - BUT: requires push-to-pull adapter (thread-safe queue between our async code and miniaudio's generator thread)\n   - Architectural mismatch with our push-based write_pcm() model\n\n3. **SoundCard** — P3\n   - Direct CoreAudio via CFFI — appealing\n   - BUT: requires float64 conversion (we produce int16)\n   - numpy required (we have it, but conversion overhead per chunk)\n\n4. **PvSpeaker** — P3\n   - Purpose-built for speech TTS — interesting niche\n   - BUT: less established, partial-write semantics, unclear asyncio story\n\n5. **rtmixer** — P4 (overkill for our needs)\n6. **PyAudio** — P4 (superseded by sounddevice)\n7. **CoreAudio ctypes** — P4 (too much implementation effort)\n8. **pygame.mixer** — NOT VIABLE (no streaming from memory)\n9. **simpleaudio** — NOT VIABLE (no streaming, unmaintained)\n\nIMPLEMENTATION PLAN:\n1. Install sounddevice: uv pip install sounddevice\n2. Create SoundDeviceStream class mirroring FFPlayStream interface\n3. Key changes:\n   - ensure_running(): open RawOutputStream(24000, 1, 'int16', latency='low'), call start()\n   - write_pcm(): use run_in_executor(None, stream.write, chunk) for async compat\n   - kill(): stream.stop() + stream.close()\n4. No silence priming needed — device is ready after start()\n5. Test: verify no truncation on first utterance after recycle\n6. Test: verify device change handling (close/reopen stream)\n\nThis also resolves speak-auf (ffplay stale audio / truncation issue).","status":"open","priority":1,"issue_type":"epic","owner":"trillium@trilliumsmith.com","created_at":"2026-02-22T19:06:05Z","created_by":"Trillium Smith","updated_at":"2026-02-22T19:09:29Z","labels":["architecture","audio"],"dependencies":[{"issue_id":"speak-9ln","depends_on_id":"speak-auf","type":"blocks","created_at":"2026-02-22T11:09:34Z","created_by":"Trillium Smith","metadata":"{}"}]}
{"id":"speak-9ln.1","title":"Evaluate sounddevice (python-sounddevice) as ffplay replacement","description":"LIBRARY: sounddevice (python-sounddevice)\nVERSION: 0.5.5 (Jan 2026) — actively maintained\nPYPI: https://pypi.org/project/sounddevice/\nDOCS: https://python-sounddevice.readthedocs.io/\n\nOVERVIEW:\nPython bindings for PortAudio. Provides RawOutputStream which accepts plain Python buffer objects (no numpy required for raw PCM). PortAudio is the gold standard cross-platform audio library — used by Audacity, VLC, and many professional audio tools.\n\nSTARTUP TIME:\n- Import loads the PortAudio shared library via CFFI. Expected ~20-50ms import time.\n- RawOutputStream() constructor opens the audio device synchronously — when it returns, the device IS ready. This is the key advantage over ffplay.\n- stream.start() begins the audio callback loop. After start() returns, write() immediately works.\n- No probing phase, no SDL initialization, no guesswork.\n\nMEMORY:\n- Package: 108.6 KB wheel (macOS universal2, includes PortAudio dylib)\n- No required dependencies beyond CFFI (already used by Kokoro)\n- NumPy is OPTIONAL — RawOutputStream/RawInputStream work without it\n- PortAudio itself is ~1-2 MB resident\n\nSTREAMING API:\n```python\nimport sounddevice as sd\n\n# Open device — synchronous, device is ready when this returns\nstream = sd.RawOutputStream(\n    samplerate=24000,\n    channels=1,\n    dtype='int16',\n    blocksize=0,  # let PortAudio choose optimal size\n    latency='low',\n)\nstream.start()\n\n# Write PCM chunks as they arrive from synthesis\n# write() blocks until buffer has space (natural backpressure)\nstream.write(pcm_chunk)  # bytes or buffer object\n\n# Check for underflow\nunderflowed = stream.write(pcm_chunk)\n\n# Non-blocking check\nframes_available = stream.write_available\n\nstream.stop()\nstream.close()\n```\n\nCan also use callback mode for even lower latency:\n```python\ndef callback(outdata, frames, time, status):\n    # Fill outdata buffer from your PCM queue\n    outdata[:] = get_next_chunk(frames)\n\nstream = sd.RawOutputStream(\n    samplerate=24000, channels=1, dtype='int16',\n    callback=callback\n)\n```\n\nmacOS ARM64: YES — universal2 wheel includes ARM64 PortAudio dylib. Works out of the box.\n\nDEVICE CHANGE DETECTION:\n- Can query sd.query_devices() to enumerate devices\n- Device index can be set per-stream\n- No built-in device-change callback, but can poll or catch stream errors\n- PortAudio on macOS uses CoreAudio backend — inherits its device management\n\nPROS:\n+ Synchronous device open — SOLVES the ffplay readiness problem\n+ Blocking write() with backpressure — natural pacing like ffplay pipe\n+ write_available for non-blocking checks\n+ No numpy dependency for raw PCM (RawOutputStream)\n+ Tiny package (108 KB), well-maintained, actively developed\n+ PortAudio is battle-tested, used everywhere\n+ Both blocking (write) and callback modes\n+ latency='low' parameter for minimal buffer sizes\n+ Works with asyncio via run_in_executor for the blocking write()\n\nCONS:\n- PortAudio adds ~1-2 MB to process memory\n- No built-in device-change notifications (must poll or catch errors)\n- Callback mode runs in a separate C thread (need thread-safe data passing)\n\nINSTALL: pip install sounddevice (or uv pip install sounddevice)\n\nVERDICT: RECOMMENDED FIRST CHOICE. Synchronous open solves our #1 problem. Blocking write provides natural backpressure. Minimal dependencies. Battle-tested PortAudio backend.","status":"open","priority":1,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-22T19:06:33Z","created_by":"Trillium Smith","updated_at":"2026-02-22T19:06:33Z","labels":["audio","research"],"dependencies":[{"issue_id":"speak-9ln.1","depends_on_id":"speak-9ln","type":"parent-child","created_at":"2026-02-22T11:06:32Z","created_by":"Trillium Smith","metadata":"{}"}]}
{"id":"speak-9ln.2","title":"Evaluate miniaudio (pyminiaudio) as ffplay replacement","description":"LIBRARY: miniaudio (pyminiaudio)\nVERSION: 1.61 (Jul 2024) — maintained but less active than sounddevice\nPYPI: https://pypi.org/project/miniaudio/\nGITHUB: https://github.com/irmen/pyminiaudio\n\nOVERVIEW:\nPython bindings for the miniaudio C library by David Reid. Single-header C library — very minimal. Uses CFFI. Supports playback, recording, and decoding. On macOS, uses CoreAudio backend directly (not through PortAudio).\n\nSTARTUP TIME:\n- Import loads compiled CFFI module. Expected ~15-40ms.\n- PlaybackDevice() opens the device synchronously via miniaudio C lib.\n- device.start(generator) begins playback with a generator callback.\n- The generator-based API means audio flows via pull model — miniaudio calls your generator when it needs data. This is inherently ready-aware: the first call to your generator IS the readiness signal.\n\nMEMORY:\n- Package: 329.7 KB wheel (macOS ARM64, includes compiled miniaudio C lib)\n- Dependencies: cffi only\n- miniaudio C lib is a single ~700KB compiled object — very lean\n- No numpy dependency\n\nSTREAMING API:\n```python\nimport miniaudio\n\ndef pcm_generator():\n    # Generator that yields PCM chunks on demand\n    # miniaudio sends the required frame count\n    required_frames = yield b''  # init\n    while True:\n        # Get PCM data from your synthesis queue\n        pcm_data = get_next_pcm(required_frames)\n        if pcm_data is None:\n            break\n        required_frames = yield pcm_data\n\ndevice = miniaudio.PlaybackDevice(\n    output_format=miniaudio.SampleFormat.SIGNED16,\n    nchannels=1,\n    sample_rate=24000,\n    buffersize_msec=100,  # internal buffer\n)\ngen = pcm_generator()\nnext(gen)  # prime the generator\ndevice.start(gen)\n# ... later\ndevice.close()\n```\n\nmacOS ARM64: YES — pre-built wheels for macOS 11.0+ ARM64 (cp38-cp312).\n\nDEVICE CHANGE DETECTION:\n- miniaudio C lib has ma_device_notification_proc for device changes\n- Python bindings may not expose this directly\n- Can enumerate devices with miniaudio.Devices()\n\nPROS:\n+ Pull-based generator model — device pulls data when ready (inherent readiness)\n+ Very lightweight — single C file compiled, no PortAudio dependency\n+ CoreAudio backend on macOS (more direct than PortAudio → CoreAudio)\n+ Small package (330 KB)\n+ No numpy dependency\n+ CFFI-based (compatible with our stack)\n+ buffersize_msec parameter for tuning latency\n\nCONS:\n- Generator-based API is pull-model — our current architecture is push-model (we have PCM and want to write it). Need an adapter: a thread-safe queue between our async write_pcm() and the generator.\n- Less mature Python bindings than sounddevice (fewer GitHub stars, fewer users)\n- Last release Jul 2024 — not as actively maintained\n- Generator approach harder to integrate with asyncio (runs on miniaudio's thread)\n- No blocking write() equivalent — must bridge push/pull via queue\n\nINSTALL: pip install miniaudio (or uv pip install miniaudio)\n\nVERDICT: STRONG SECOND CHOICE. The pull-based generator model is elegant but requires architectural adaptation. The push-to-pull bridging adds complexity. If sounddevice doesn't work out, miniaudio is the next best option.","status":"open","priority":2,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-22T19:06:56Z","created_by":"Trillium Smith","updated_at":"2026-02-22T19:06:56Z","labels":["audio","research"],"dependencies":[{"issue_id":"speak-9ln.2","depends_on_id":"speak-9ln","type":"parent-child","created_at":"2026-02-22T11:06:56Z","created_by":"Trillium Smith","metadata":"{}"}]}
{"id":"speak-9ln.3","title":"Evaluate SoundCard as ffplay replacement","description":"LIBRARY: SoundCard\nVERSION: 0.4.5 (Sep 2025) — maintained\nPYPI: https://pypi.org/project/SoundCard/\nGITHUB: https://github.com/bastibe/SoundCard\n\nOVERVIEW:\nPure-Python (CFFI) audio library that talks directly to native OS audio APIs — CoreAudio on macOS, PulseAudio on Linux, WASAPI on Windows. No PortAudio middleman. Written by Bastian Bechtold (same author as the 'Audio APIs' blog series).\n\nSTARTUP TIME:\n- Import loads CFFI bindings to CoreAudio. Expected ~20-40ms.\n- speaker.player(samplerate=24000) opens a context manager. Device is ready when the context is entered.\n- Synchronous open — same advantage as sounddevice.\n\nMEMORY:\n- Package: 43.9 KB wheel (pure Python + CFFI, no compiled extensions)\n- Dependencies: cffi, numpy (REQUIRED — not optional)\n- Talks to CoreAudio directly via CFFI — no PortAudio layer\n\nSTREAMING API:\n```python\nimport soundcard as sc\nimport numpy as np\n\nspeaker = sc.default_speaker()\nwith speaker.player(samplerate=24000, channels=1, blocksize=1024) as player:\n    # Device is ready when context manager enters\n    # Write chunks as they arrive\n    # Data must be numpy float64 array scaled to [-1, 1]\n    pcm_int16 = np.frombuffer(raw_pcm, dtype=np.int16)\n    pcm_float = pcm_int16.astype(np.float64) / 32768.0\n    player.play(pcm_float)\n    # Successive play() calls queue audio\n```\n\nmacOS ARM64: YES — pure Python wheel (py3-none-any), CFFI generates bindings at runtime. No pre-compiled platform-specific code needed.\n\nDEVICE CHANGE DETECTION:\n- Can query sc.all_speakers() to enumerate devices\n- Direct CoreAudio access means potential for device notifications\n- No built-in callback for device changes\n\nPROS:\n+ Direct CoreAudio on macOS — no PortAudio middleman\n+ Tiny package (44 KB)\n+ Synchronous device open via context manager\n+ Successive play() calls queue audio naturally\n+ Active maintenance (Sep 2025 release)\n+ blocksize parameter for latency tuning\n\nCONS:\n- REQUIRES numpy (not optional) — though we already have it for Kokoro\n- Requires float64 conversion — our PCM is int16, need to convert each chunk (CPU overhead)\n- Data must be normalized to [-1, 1] float range\n- Less widely used than sounddevice/PortAudio\n- The float conversion adds latency and memory overhead per chunk\n- play() semantics are less well-documented for streaming than sounddevice write()\n- Context manager pattern means device lifecycle is tied to a with block\n\nINSTALL: pip install soundcard (or uv pip install soundcard)\n\nVERDICT: VIABLE BUT LESS IDEAL. The float64 conversion requirement adds per-chunk overhead. numpy dependency is not a problem for us (already have it), but the data format mismatch (we produce int16, it wants float64) is a friction point. Direct CoreAudio is appealing but doesn't provide enough advantage over PortAudio to justify the conversion overhead.","status":"open","priority":3,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-22T19:07:17Z","created_by":"Trillium Smith","updated_at":"2026-02-22T19:07:17Z","labels":["audio","research"],"dependencies":[{"issue_id":"speak-9ln.3","depends_on_id":"speak-9ln","type":"parent-child","created_at":"2026-02-22T11:07:16Z","created_by":"Trillium Smith","metadata":"{}"}]}
{"id":"speak-9ln.4","title":"Evaluate rtmixer (python-rtmixer) as ffplay replacement","description":"LIBRARY: rtmixer (python-rtmixer)\nVERSION: 0.1.7 (Jul 2024)\nPYPI: https://pypi.org/project/rtmixer/\nGITHUB: https://github.com/spatialaudio/python-rtmixer\nDOCS: https://python-rtmixer.readthedocs.io/\n\nOVERVIEW:\nBuilt on top of sounddevice (PortAudio). Provides a C-level audio callback that bypasses the Python GIL entirely. Uses a RingBuffer for lock-free data passing between Python and the C audio thread. Designed for applications where Python's GIL would cause audio glitches.\n\nSTARTUP TIME:\n- Depends on sounddevice for device management — same startup characteristics\n- Additional overhead from CFFI-compiled C callback setup\n- Expected ~30-60ms import + device open\n\nMEMORY:\n- Package: 21.2 KB (source) + sounddevice dependency\n- Combined: ~130 KB for rtmixer + sounddevice wheels\n- Additional PortAudio dylib from sounddevice\n\nSTREAMING API:\n```python\nimport rtmixer\nimport sounddevice as sd\n\nmixer = rtmixer.Mixer(\n    samplerate=24000,\n    channels=1,\n    dtype='int16',\n    latency='low',\n)\nmixer.start()\n\n# Write via ring buffer (lock-free)\naction = mixer.play_buffer(pcm_data, channels=1)\n# action object tracks completion\n\n# Or use RingBuffer for streaming\nringbuf = rtmixer.RingBuffer(elementsize=2, size=24000)\naction = mixer.play_ringbuffer(ringbuf)\n# Write to ringbuf from Python thread\nringbuf.write(pcm_chunk)\n```\n\nmacOS ARM64: YES — wheels available for macOS 11.0+ ARM64.\n\nDEVICE CHANGE DETECTION:\n- Same as sounddevice (inherits PortAudio device management)\n\nPROS:\n+ C-level callback bypasses GIL — most reliable for glitch-free audio\n+ Lock-free RingBuffer for streaming\n+ Built on sounddevice — inherits all its advantages\n+ play_buffer() for one-shot playback, play_ringbuffer() for streaming\n+ Mixer class — can overlay multiple audio sources\n\nCONS:\n- OVERKILL for our use case — we don't have GIL contention issues (synthesis runs in executor thread, playback is the only thing in the event loop)\n- Extra dependency layer (rtmixer + sounddevice + CFFI)\n- Less intuitive API than plain sounddevice\n- 'Work in progress' status noted in README\n- RingBuffer adds complexity vs simple write()\n- Less documentation and examples than sounddevice alone\n\nINSTALL: pip install rtmixer (or uv pip install rtmixer)\n\nVERDICT: OVERKILL. The GIL-bypassing C callback is impressive but unnecessary for our architecture. We run synthesis in a thread pool executor and playback is I/O-bound, not CPU-bound. Plain sounddevice with blocking write() is simpler and sufficient. Only consider if we encounter GIL-related audio glitches with sounddevice.","status":"open","priority":4,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-22T19:07:35Z","created_by":"Trillium Smith","updated_at":"2026-02-22T19:07:35Z","labels":["audio","research"],"dependencies":[{"issue_id":"speak-9ln.4","depends_on_id":"speak-9ln","type":"parent-child","created_at":"2026-02-22T11:07:35Z","created_by":"Trillium Smith","metadata":"{}"}]}
{"id":"speak-9ln.5","title":"Evaluate PvSpeaker as ffplay replacement","description":"LIBRARY: PvSpeaker\nVERSION: latest on PyPI (Picovoice)\nGITHUB: https://github.com/Picovoice/pvspeaker\nLICENSE: Apache-2.0 (free, open source)\n\nOVERVIEW:\nCross-platform audio player by Picovoice, designed specifically for real-time speech audio processing. Streams raw PCM directly to system audio output. Written in C with Python bindings. Supports Windows, macOS, Linux, Raspberry Pi.\n\nSTARTUP TIME:\n- Loads compiled C library. Expected ~10-30ms.\n- PvSpeaker(sample_rate, bits_per_sample, device_index) — constructor initializes device\n- speaker.start() begins the playback stream\n- Not clear if start() is synchronous or if there's a readiness signal\n\nMEMORY:\n- Package size: not confirmed from PyPI (page rendering issue)\n- Includes compiled C library for each platform\n- Minimal dependencies — purpose-built for speech\n\nSTREAMING API:\n```python\nfrom pvspeaker import PvSpeaker\n\nspeaker = PvSpeaker(\n    sample_rate=24000,\n    bits_per_sample=16,\n    device_index=0,  # or -1 for default\n)\nspeaker.start()\n\n# Write PCM chunks — returns number of samples written\n# Only writes as much as internal circular buffer can fit\nwritten = speaker.write(pcm_chunk)\n\nspeaker.stop()\nspeaker.delete()\n```\n\nmacOS ARM64: LIKELY YES — Picovoice products target Apple Silicon. But need to verify wheel availability.\n\nDEVICE CHANGE DETECTION:\n- Can enumerate devices with PvSpeaker.get_available_devices()\n- No documented device-change callback\n\nPROS:\n+ Purpose-built for speech TTS playback — exactly our use case\n+ write() returns bytes written — implicit buffer management\n+ Apache-2.0 license\n+ Minimal API surface — easy to integrate\n+ Cross-platform C library — likely fast\n\nCONS:\n- Picovoice ecosystem dependency — small company, less community than PortAudio\n- write() partial-write semantics need a loop (like POSIX write())\n- No documented readiness signal after start()\n- Less mature/established than sounddevice/PortAudio\n- Mono-only (fine for us, but limits future flexibility)\n- PyPI page had rendering issues — concerning for package health\n- Unclear asyncio integration story\n\nINSTALL: pip install pvspeaker (or uv pip install pvspeaker)\n\nVERDICT: INTERESTING NICHE OPTION. Purpose-built for speech is appealing, but less established than PortAudio-based options. The partial-write semantics add complexity. Consider if sounddevice and miniaudio both fail. Need to verify macOS ARM64 wheel availability.","status":"open","priority":3,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-22T19:07:53Z","created_by":"Trillium Smith","updated_at":"2026-02-22T19:07:53Z","labels":["audio","research"],"dependencies":[{"issue_id":"speak-9ln.5","depends_on_id":"speak-9ln","type":"parent-child","created_at":"2026-02-22T11:07:53Z","created_by":"Trillium Smith","metadata":"{}"}]}
{"id":"speak-9ln.6","title":"Evaluate PyAudio as ffplay replacement","description":"LIBRARY: PyAudio\nVERSION: 0.2.14 (Nov 2023) — slow release cadence\nPYPI: https://pypi.org/project/PyAudio/\nDOCS: https://people.csail.mit.edu/pyaudio/docs/\n\nOVERVIEW:\nThe original Python PortAudio bindings. Lower-level than sounddevice, using CPython extensions (not CFFI). Provides both callback and blocking modes. Has been around since ~2006.\n\nSTARTUP TIME:\n- Import loads the CPython extension module. Expected ~10-30ms.\n- p = pyaudio.PyAudio() initializes PortAudio.\n- stream = p.open(...) opens the device synchronously.\n- After open() returns, the device IS ready — same readiness guarantee as sounddevice.\n\nMEMORY:\n- Package: 47.1 KB source distribution\n- NO macOS wheels — must compile from source with Homebrew portaudio\n- PortAudio dylib: ~1-2 MB\n\nSTREAMING API:\n```python\nimport pyaudio\n\np = pyaudio.PyAudio()\nstream = p.open(\n    format=pyaudio.paInt16,\n    channels=1,\n    rate=24000,\n    output=True,\n    frames_per_buffer=1024,\n)\n\n# Write chunks — blocking until buffer accepts data\nstream.write(pcm_chunk)\n\nstream.stop_stream()\nstream.close()\np.terminate()\n```\n\nmacOS ARM64: PROBLEMATIC — no pre-built wheels. Must install portaudio via Homebrew first, then pip install PyAudio compiles from source. This is fragile:\n  brew install portaudio\n  pip install pyaudio\n\nDEVICE CHANGE DETECTION:\n- Can enumerate devices with p.get_device_count() / p.get_device_info_by_index()\n- No device-change callback\n\nPROS:\n+ Synchronous device open — same readiness guarantee as sounddevice\n+ Blocking write() with backpressure\n+ Low-level PortAudio access for fine-tuning\n+ Long history, well-understood behavior\n+ Simple API for basic streaming\n\nCONS:\n- NO macOS wheels — requires Homebrew portaudio + compilation\n- CPython extension (not CFFI) — less compatible with alternative Python implementations\n- Slower release cadence (last release Nov 2023)\n- More verbose API than sounddevice (need PyAudio() manager object)\n- Less Pythonic than sounddevice\n- Installation is fragile on macOS — portaudio version mismatches cause build failures\n- sounddevice is strictly better for our use case (same PortAudio, better API, better packaging)\n\nINSTALL:\n  brew install portaudio\n  pip install pyaudio\n\nVERDICT: SUPERSEDED BY SOUNDDEVICE. Same PortAudio backend, but worse packaging (no macOS wheels), more verbose API, and fragile installation. sounddevice provides everything PyAudio does with better ergonomics and bundled PortAudio. No reason to choose PyAudio over sounddevice in 2026.","status":"open","priority":4,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-22T19:08:10Z","created_by":"Trillium Smith","updated_at":"2026-02-22T19:08:10Z","labels":["audio","research"],"dependencies":[{"issue_id":"speak-9ln.6","depends_on_id":"speak-9ln","type":"parent-child","created_at":"2026-02-22T11:08:09Z","created_by":"Trillium Smith","metadata":"{}"}]}
{"id":"speak-9ln.7","title":"Evaluate CoreAudio via ctypes/PyObjC as ffplay replacement","description":"LIBRARY: CoreAudio via ctypes/CFFI/PyObjC (custom wrapper)\nVERSION: N/A — would be custom code wrapping macOS frameworks\n\nOVERVIEW:\nWrite direct Python bindings to Apple's CoreAudio framework using ctypes, CFFI, or PyObjC. This eliminates all intermediary layers (PortAudio, SDL, etc.) and talks directly to the macOS audio subsystem. Reference implementation: SoundCard's coreaudio.py (github.com/bastibe/SoundCard/blob/master/soundcard/coreaudio.py).\n\nSTARTUP TIME:\n- AudioUnit/AudioQueue initialization is fast (~5-15ms for device open)\n- CoreAudio is always loaded by the OS — no library loading overhead\n- Potentially the fastest option since there's zero abstraction overhead\n- AudioUnit provides kAudioUnitRenderAction_OutputIsSilence and render callbacks that signal exact readiness\n\nMEMORY:\n- Zero additional dependencies — CoreAudio frameworks are part of macOS\n- Only ctypes/CFFI for the bridge layer\n- Smallest possible footprint\n\nSTREAMING API (conceptual — AudioQueue approach):\n```python\nimport ctypes\nfrom ctypes import c_void_p, c_uint32, byref\n# Load AudioToolbox framework\nat = ctypes.cdll.LoadLibrary('/System/Library/Frameworks/AudioToolbox.framework/AudioToolbox')\n\n# Create AudioQueue for output\n# Set up AudioStreamBasicDescription for 24kHz mono int16\n# Register callback for buffer completion\n# Allocate buffers, prime, start queue\n# Callback fires when buffer consumed — fill with next PCM chunk\n```\n\nMore realistically, use PyObjC:\n```python\nimport CoreAudio  # via PyObjC\n# Use AudioUnit or AVAudioEngine APIs\n```\n\nmacOS ARM64: YES — native frameworks, always available.\n\nDEVICE CHANGE DETECTION:\n+ BEST OPTION — CoreAudio has AudioObjectAddPropertyListener for kAudioHardwarePropertyDefaultOutputDevice\n+ Can detect device plugged/unplugged/changed in real-time\n+ This is how all native macOS apps handle device changes\n\nPROS:\n+ Zero dependencies — uses OS frameworks only\n+ Potentially lowest latency (no abstraction layers)\n+ Best device-change detection (native notifications)\n+ Smallest memory footprint\n+ Full control over buffer sizes and callback timing\n+ No pip install required\n\nCONS:\n- ENORMOUS implementation effort — CoreAudio is notoriously hard to use\n- CoreAudio documentation is described as 'horrible' by experienced developers\n- Must handle C struct layouts, callback conventions, memory management manually\n- AudioUnit/AudioQueue APIs are complex and poorly documented\n- macOS-only — loses cross-platform capability if we ever port\n- No community support — we own all bugs\n- ctypes bridges to C callbacks are fragile (GC can collect callback references)\n- Would need to rewrite SoundCard's coreaudio.py level of code (~500+ lines)\n- Testing and debugging C-level audio code from Python is painful\n\nINSTALL: No installation needed (built into macOS). For PyObjC: pip install pyobjc-framework-CoreAudio\n\nVERDICT: NOT RECOMMENDED unless all library options fail. The implementation effort is disproportionate to the benefit. PortAudio (via sounddevice) already uses CoreAudio under the hood on macOS — we get 95% of the performance with 5% of the code. The only unique advantage is native device-change notifications, but we can poll or catch errors as a workaround. Reserve this as a last resort or future enhancement for the device-change feature specifically.","status":"open","priority":4,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-22T19:08:36Z","created_by":"Trillium Smith","updated_at":"2026-02-22T19:08:36Z","labels":["audio","research"],"dependencies":[{"issue_id":"speak-9ln.7","depends_on_id":"speak-9ln","type":"parent-child","created_at":"2026-02-22T11:08:36Z","created_by":"Trillium Smith","metadata":"{}"}]}
{"id":"speak-9ln.8","title":"Evaluate pygame.mixer as ffplay replacement","description":"LIBRARY: pygame.mixer\nVERSION: 2.6.1 (pygame) / 2.5.7 (pygame-ce)\nPYPI: https://pypi.org/project/pygame/ or https://pypi.org/project/pygame-ce/\n\nOVERVIEW:\nSDL2-based audio mixer from the pygame game framework. Provides Sound objects and channel-based mixing. Designed for game audio with pre-loaded sound effects and music streaming.\n\nSTARTUP TIME:\n- pygame.init() or pygame.mixer.init() initializes SDL2 audio subsystem\n- SDL2 initialization includes video subsystem by default (can use mixer.init() alone)\n- Expected ~50-100ms for full init, ~20-40ms for mixer-only init\n- SDL2 audio device open is similar to ffplay's SDL — same initialization overhead we're trying to escape\n\nMEMORY:\n- Package: ~10-15 MB (full pygame with SDL2 bundled)\n- pygame-ce is similar size\n- Massive overkill — we'd be installing a game framework for audio playback\n\nSTREAMING API:\n```python\nimport pygame.mixer\n\npygame.mixer.init(frequency=24000, size=-16, channels=1, buffer=1024)\n\n# Option 1: Sound objects (must have full buffer)\nsound = pygame.mixer.Sound(buffer=pcm_bytes)\nsound.play()\n\n# Option 2: Music streaming from file (not from memory/pipe)\npygame.mixer.music.load('audio.wav')\npygame.mixer.music.play()\n```\n\nmacOS ARM64: YES — pygame wheels include SDL2 for ARM64.\n\nDEVICE CHANGE DETECTION:\n- No built-in device change detection\n- SDL2 has limited device change support\n\nPROS:\n+ Widely used, well-maintained\n+ Simple Sound.play() API for one-shot playback\n+ macOS ARM64 wheels available\n\nCONS:\n- DOES NOT SUPPORT STREAMING from memory chunks — Sound() requires complete buffer, music.load() requires a file\n- SDL2 initialization is the SAME overhead as ffplay (ffplay uses SDL2!)\n- Would not solve the readiness problem — SDL2 device open has the same asynchronous startup\n- Massive package size (~10-15 MB) for audio-only use\n- Game-oriented API, not designed for speech TTS streaming\n- Cannot write PCM chunks incrementally\n- Would need to buffer entire utterance before playback (defeats streaming purpose)\n\nINSTALL: pip install pygame (or uv pip install pygame)\n\nVERDICT: NOT VIABLE. Does not support streaming from memory chunks — requires complete audio buffers or files. Uses SDL2 which has the same initialization timing issues as ffplay. Massive package for our needs. Fundamentally wrong tool for the job.","status":"open","priority":4,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-22T19:08:54Z","created_by":"Trillium Smith","updated_at":"2026-02-22T19:08:54Z","labels":["audio","research"],"dependencies":[{"issue_id":"speak-9ln.8","depends_on_id":"speak-9ln","type":"parent-child","created_at":"2026-02-22T11:08:53Z","created_by":"Trillium Smith","metadata":"{}"}]}
{"id":"speak-9ln.9","title":"Evaluate simpleaudio as ffplay replacement","description":"LIBRARY: simpleaudio\nVERSION: 1.0.4 (last release ~2020) — appears unmaintained\nPYPI: https://pypi.org/project/simpleaudio/\nDOCS: https://simpleaudio.readthedocs.io/\n\nOVERVIEW:\nSimple, asynchronous audio playback for Python 3. Plays audio from NumPy arrays, Python bytes, or wave files. Uses CoreAudio on macOS, WinMM on Windows, ALSA on Linux. Pure C extension, no PortAudio.\n\nSTARTUP TIME:\n- Import loads CPython extension. Expected ~5-15ms.\n- play_buffer() opens device, plays, returns immediately (non-blocking)\n- WaveObject.play() similar behavior\n- Device open is part of play — not separable\n\nMEMORY:\n- Package: small (~50 KB)\n- No dependencies for basic use\n- Uses native OS audio APIs directly\n\nSTREAMING API:\n```python\nimport simpleaudio as sa\n\n# Play complete buffer — NOT streaming\nplay_obj = sa.play_buffer(\n    pcm_bytes,\n    num_channels=1,\n    bytes_per_sample=2,\n    sample_rate=24000,\n)\nplay_obj.wait_done()  # blocks until complete\n```\n\nmacOS ARM64: UNCERTAIN — no ARM64-specific wheels listed. May need to compile from source. Package appears unmaintained.\n\nDEVICE CHANGE DETECTION:\n- None\n\nPROS:\n+ Very simple API\n+ Uses CoreAudio directly on macOS\n+ Small package, no dependencies\n+ play_buffer() is straightforward\n\nCONS:\n- NO STREAMING SUPPORT — must provide complete audio buffer to play_buffer()\n- Cannot write chunks incrementally\n- play_buffer() requires full PCM data upfront\n- Appears UNMAINTAINED (last release ~2020)\n- No ARM64 wheels\n- No readiness callback or synchronous open\n- No way to queue audio or append to playing stream\n- Would need to buffer entire utterance (defeats streaming purpose)\n\nINSTALL: pip install simpleaudio\n\nVERDICT: NOT VIABLE. Does not support streaming — requires complete audio buffer. Unmaintained since ~2020. No ARM64 wheels. Same fundamental problem as pygame.mixer for our use case.","status":"open","priority":4,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-22T19:09:08Z","created_by":"Trillium Smith","updated_at":"2026-02-22T19:09:08Z","labels":["audio","research"],"dependencies":[{"issue_id":"speak-9ln.9","depends_on_id":"speak-9ln","type":"parent-child","created_at":"2026-02-22T11:09:08Z","created_by":"Trillium Smith","metadata":"{}"}]}
{"id":"speak-auf","title":"Investigate ffplay stale audio issue","description":"ffplay truncates the beginning of audio on first utterance after being spawned.\n\nRoot cause: When ffplay starts, it goes through three phases before audio actually plays:\n1. SDL2 audio device open — SDL negotiates with CoreAudio, allocates buffers (takes real time)\n2. Probe phase — ffplay reads -probesize bytes from stdin to detect format (even with explicit -f/-ar flags)\n3. SDL audio callback starts — only now does ffplay pull from its buffer and send to speakers\n\nBetween phases 2 and 3, ffplay's stdin pipe is writable. Our code writes PCM, drain() succeeds (kernel pipe buffer accepts it), and ffplay reads it into its internal ring buffer. But SDL hasn't started playback yet. When SDL finally opens the audio device and fires its first callback, some of the earliest samples written during this window are partially or fully lost.\n\nThe 100ms silence prime in ensure_running() helps by giving ffplay something to consume during initialization, but if real audio arrives before SDL is fully ready, the speech onset still gets eaten.\n\nSymptom: first utterance after ffplay recycle (queue drains → kill → next enqueue respawns) has beginning truncated. Subsequent utterances in same batch play fine since ffplay is already initialized.\n\nPossible fixes:\n- Increase prime buffer (200-300ms) and await a short sleep after priming\n- Pre-spawn ffplay on daemon start, keep it warm\n- Use -fflags +nobuffer to skip ffplay's internal buffering\n- Replace ffplay with direct CoreAudio output (pyaudio/sounddevice) — eliminates the middleman entirely\n- Write a ready-detection: send silence, measure when drain() starts blocking (means SDL is consuming), then send real audio","notes":"2026-02-22: Research epic created: speak-9ln (Replace ffplay with direct audio playback library). Completing that epic resolves this issue — the root cause is ffplay's lack of readiness signaling, and all viable replacement libraries provide synchronous device open. Recommended solution: sounddevice with RawOutputStream.","status":"open","priority":1,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-21T00:27:25Z","created_by":"Trillium Smith","updated_at":"2026-02-22T19:10:06Z","labels":["bug","reliability","research-finding"]}
{"id":"speak-enf","title":"Add Claude Code hook integration","description":"Multiple competing projects (clarvis, cc-hooks, multi-agent-observability, Benny Cheung) integrate with Claude Code via hooks. We should add hook scripts that capture Claude Code lifecycle events (especially Stop/Notification) and pipe relevant text to speak --enqueue. This is the primary way agents would automatically speak without manual speak --enqueue calls. The auto-detect caller feature already identifies the git repo, so hooks would automatically get proper caller identification.","notes":"Implemented in .claude/hooks/speak-hook.sh. Hooks configured in ~/.claude/settings.json for Stop, Notification, and SubagentStop events. All async. Uses bin/summarize (Haiku) + bin/speak-summarize (pronunciation/phrase rewrites) pipeline.","status":"closed","priority":2,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-21T00:27:17Z","created_by":"Trillium Smith","updated_at":"2026-02-21T01:19:44Z","closed_at":"2026-02-21T01:19:44Z","labels":["enhancement","integration","research-finding"]}
{"id":"speak-f26","title":"Enforce semantic atomic commit messages via git hook","description":"Add commit-msg hook enforcing conventional commit format (feat:, fix:, refactor:, docs:, etc.). Optionally warn when too many unrelated files are staged. Research in progress.","status":"done","priority":2,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-22T06:40:18Z","created_by":"Trillium Smith","updated_at":"2026-02-22T06:44:24Z","labels":["code-quality","tooling"]}
{"id":"speak-fjr","title":"Add pronunciation dictionary and phrase rewrite system","description":"Implemented config/rewrites.json with two sections: pronunciation fixes and phrase rewrites. Applied by bin/speak-summarize.","status":"closed","priority":2,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-21T01:19:58Z","created_by":"Trillium Smith","updated_at":"2026-02-21T01:20:03Z","closed_at":"2026-02-21T01:20:03Z","close_reason":"Closed","labels":["enhancement"]}
{"id":"speak-jr7","title":"Add LLM summarization of verbose output before speaking","description":"Agent-tts and clarvis both use Claude Haiku or similar LLMs to condense verbose agent output into 1-3 sentence spoken summaries. This makes audio notifications actually useful rather than reading raw verbose text. Could be a thin wrapper that calls an LLM and pipes result to speak --enqueue. Inspired by: agent-tts (message-processor.ts haiku() function) and clarvis (Jarvis-style summarization modes: terse/brief/normal/full).","notes":"Implemented as bin/summarize standalone tool. Uses claude -p with Haiku. Short text passes through unchanged. Swappable backend by editing one file. Wired into hook pipeline: summarize → speak-summarize → speak --enqueue.","status":"closed","priority":3,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-21T00:26:38Z","created_by":"Trillium Smith","updated_at":"2026-02-21T01:19:50Z","closed_at":"2026-02-21T01:19:50Z","labels":["enhancement","research-finding"]}
{"id":"speak-kv3","title":"Fast enqueue client: C binary or Perl for hot path","description":"Research complete. Current Python speak-client adds ~16ms startup per enqueue call. Three options benchmarked: (1) C binary speak-enqueue at 3.5ms, (2) Perl one-liner at 9ms using system perl, (3) printf+nc at 4.3ms fire-and-forget. Recommendation: compile C binary for enqueue hot path, keep Python client for streaming/interactive. See task a3b8c95 output for full code examples.","status":"open","priority":3,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-21T01:34:55Z","created_by":"Trillium Smith","updated_at":"2026-02-21T01:34:55Z","labels":["enhancement","performance"]}
{"id":"speak-kxp","title":"Pipeline render_speech() with producer/consumer pattern","description":"Replace sequential synthesis loop in lib/speakd/renderer.py with asyncio.Queue producer/consumer. Producer synthesizes clauses ahead, consumer writes PCM to ffplay. Eliminates audible gaps between clauses. Single-file change, ~120 lines.","status":"done","priority":1,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-22T06:40:17Z","created_by":"Trillium Smith","updated_at":"2026-02-22T06:43:01Z","labels":["enhancement","performance"]}
{"id":"speak-poq","title":"Auto-suggest phonetic rewrites for unknown words","description":"When speak encounters a word without a pronunciation rewrite, auto-generate a candidate using eng-to-ipa (CMU dict) with espeak-ng fallback. Would need an IPA-to-phonetic-spelling translation layer since Kokoro takes plain text, not IPA. Queue candidates into rewrites-review.json as pending for user review. See also g2p-en for neural fallback on novel words.","status":"open","priority":4,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-21T02:35:52Z","created_by":"Trillium Smith","updated_at":"2026-02-21T02:35:52Z","labels":["enhancement","rewrites"]}
{"id":"speak-pwo","title":"Rethink backpressure: pre-synthesize all clauses, queue PCM on disk","description":"Current design uses ffplay backpressure to pace writes. User clarification: we don't care about buffering constraints. We should synthesize all clauses as fast as possible, save PCM to filesystem, then play them sequentially. This eliminates the pipeline complexity and the gap problem entirely — all audio is ready before playback starts (or at least clause N+1 is ready before clause N finishes). The bounded asyncio.Queue and chunked drain() writes are over-engineered for our use case.","status":"open","priority":2,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-22T06:53:30Z","created_by":"Trillium Smith","updated_at":"2026-02-22T06:53:30Z","labels":["architecture","enhancement"]}
{"id":"speak-sm9","title":"Audible gap between caller tone and speech: TTS not pipelined with tone playback","description":"## Problem\nUser hears: tone, then noticeable pause, then voice starts speaking. The tone should flow seamlessly into speech with no gap.\n\n## Root cause\nThe TTS synthesis pipeline runs entirely AFTER the caller tone finishes playing. Nothing is pipelined. The sequence in playback.py _worker (lines 187-211):\n\n1. write_pcm(caller_tone) blocks until tone PCM is fully written to PortAudio\n2. self._publish('playing') does synchronous disk I/O (writes JSON state file)\n3. render_speech() entered, setup logging, log_event (more disk I/O)\n4. kokoro.create_stream() called: phonemize(text) runs espeak (non-trivial latency), then ONNX inference for first chunk runs in executor thread\n\nOnly after ALL of that does the first audio chunk reach PortAudio. The gap is dominated by phonemization + first-chunk ONNX inference (estimated 100-300ms depending on text length), plus minor contributions from synchronous file I/O in publish_state and log_event.\n\n## Code paths involved\n- lib/speakd/playback.py _worker(): lines 187-211 (sequential tone then publish then render)\n- lib/speakd/renderer.py render_speech(): lines 47-53 setup, line 59 create_stream\n- kokoro_onnx create_stream(): phonemize (line 232) + _create_audio in executor (line 242)\n- lib/speakd/protocol.py publish_state(): synchronous file write\n\n## Suggested fix\nPipeline synthesis with tone playback. Start TTS work BEFORE or DURING tone playback:\n\n1. Pre-phonemize: Call kokoro.tokenizer.phonemize() before writing the tone. Phonemization is pure CPU that does not need the audio device.\n\n2. Overlap first-chunk inference with tone playback: Launch the create_stream background task (or at minimum the phonemize + first _create_audio call) concurrently with write_pcm(tone). Use asyncio.gather or start synthesis, then await tone write, then await first chunk.\n\n3. Move publish_state off critical path: Fire-and-forget via asyncio.create_task or move after first audio chunk. Or use async file I/O.\n\n4. Move log_event off critical path: Same treatment, append-only writes that don't need to block audio.\n\nRough sketch of pipelined approach in _worker:\n  synthesis_iter = synth.kokoro.create_stream(text, voice, speed, lang)\n  prefetch = asyncio.create_task(anext(synthesis_iter))\n  await self._ffplay.write_pcm(get_caller_tone(caller))\n  first_chunk = await prefetch  # likely already done while tone was playing\n  self._publish('playing')\n  await self._ffplay.write_pcm(first_chunk_pcm)\n  async for remaining chunks...","status":"closed","priority":2,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-22T23:26:01Z","created_by":"Trillium Smith","updated_at":"2026-02-22T23:32:39Z","closed_at":"2026-02-22T23:32:39Z","close_reason":"Closed","labels":["latency","playback","ux"]}
{"id":"speak-wei","title":"Add priority tiers to voice pool","description":"Refactor VoicePool to support priority tiers. Split voices into preferred and fallback lists in config/voices.json. Pool draws from preferred first, only uses fallback when preferred are exhausted. Add CLI commands --voice-prefer and --voice-demote to move voices between tiers.","status":"open","priority":2,"issue_type":"task","owner":"trillium@trilliumsmith.com","created_at":"2026-02-22T21:50:43Z","created_by":"Trillium Smith","updated_at":"2026-02-22T21:50:43Z","labels":["enhancement","voice-pool"]}
