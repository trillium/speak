#!/usr/bin/env python3
"""Benchmark: How long does Kokoro take to yield the first audio chunk?

Tests whether the audible gap between caller tone and speech is caused by:
  A) Kokoro synthesizing the entire utterance as one chunk (our hypothesis)
  B) Slow phonemization (espeak call)
  C) Slow ONNX inference regardless of text length
  D) Something else entirely

Run:
  uv run --python 3.14 --with kokoro-onnx -- python3 bin/bench-first-chunk

What to look for:
  - If 'tiny' and 'long' have similar first_chunk times → it's not chunk size
  - If phonemize is most of the time → espeak is the bottleneck
  - If first_chunk scales linearly with phoneme count → ONNX inference scales with length
  - If first_chunk is fast here but slow in daemon → the daemon adds overhead (cache lookup, voice pool, etc.)

Compare results against daemon prefetch times from logs:
  grep 'prefetch_first_chunk DONE' /tmp/speak-$USER.sock.log
"""

import asyncio
import sys
import time

# Add project root to path so we can import the Kokoro speed monkeypatch
sys.path.insert(0, __import__("os").path.join(__import__("os").path.dirname(__file__), "..", "lib"))

MODEL = "~/.local/share/speak/kokoro/kokoro-v1.0.onnx"
VOICES = "~/.local/share/speak/kokoro/voices-v1.0.bin"
VOICE = "af_heart"
SPEED = 1.26
LANG = "en-us"

TESTS = [
    ("tiny",   "Hello."),
    ("short",  "Testing one two three."),
    ("medium", "The quick brown fox jumps over the lazy dog and runs away."),
    ("long",   "Filed as speak 788, priority 2. Five possible approaches listed: "
               "extend the tone dynamically until speech is ready, pre-phonemize "
               "at enqueue time, reduce the first chunk size, warm the cache in "
               "background, or fill the gap with an intentional sound."),
]


async def bench():
    import os
    from kokoro_onnx import Kokoro
    from speakd.kokoro_patch import apply_patch

    # Apply speed dtype monkeypatch (np.int32 → np.float32)
    apply_patch()

    model = os.path.expanduser(MODEL)
    voices = os.path.expanduser(VOICES)

    print("Loading model...")
    t0 = time.monotonic()
    k = Kokoro(model, voices)
    print(f"Model loaded in {(time.monotonic() - t0) * 1000:.0f}ms\n")

    print(f"{'test':8s} {'phonemes':>8s} {'phonemize':>10s} {'1st_chunk':>10s} "
          f"{'1st_audio':>10s} {'chunks':>6s} {'total':>8s}")
    print("-" * 72)

    for label, text in TESTS:
        # Step 1: Time phonemization alone (espeak)
        t0 = time.monotonic()
        phonemes = k.tokenizer.phonemize(text, LANG)
        t_phonemize = (time.monotonic() - t0) * 1000

        # Step 2: Time first chunk from create_stream
        t0 = time.monotonic()
        stream = k.create_stream(text, VOICE, SPEED, LANG, trim=False)
        chunks = 0
        first_dur = 0.0
        t_first = 0.0
        async for audio, sr in stream:
            if chunks == 0:
                t_first = (time.monotonic() - t0) * 1000
                first_dur = len(audio.squeeze()) / sr
            chunks += 1
        t_total = (time.monotonic() - t0) * 1000

        print(f"{label:8s} {len(phonemes):8d} {t_phonemize:9.0f}ms {t_first:9.0f}ms "
              f"{first_dur:9.1f}s {chunks:6d} {t_total:7.0f}ms")

    # Step 3: Run tiny a second time to see if warm cache / JIT helps
    print("\n--- Warm run (tiny, repeated) ---")
    for i in range(3):
        text = "Hello."
        t0 = time.monotonic()
        async for audio, sr in k.create_stream(text, VOICE, SPEED, LANG, trim=False):
            t_first = (time.monotonic() - t0) * 1000
            break
        print(f"  run {i+1}: first_chunk={t_first:.0f}ms")


if __name__ == "__main__":
    asyncio.run(bench())
